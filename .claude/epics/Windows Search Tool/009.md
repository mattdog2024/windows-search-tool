---
name: DeepSeek AI 服务集成
status: open
created: 2025-10-16T08:21:20Z
updated: 2025-10-16T08:21:20Z
github: Will be updated when synced to GitHub
depends_on: [003]
parallel: true
conflicts_with: []
---

# Task: DeepSeek AI 服务集成

## Description

集成 DeepSeek AI 服务,为文档提供智能摘要生成和关键信息提取功能。实现在线状态检测、API 调用、结果缓存和数据库存储,提升文档预览和搜索体验。支持配置化管理,可选启用/禁用 AI 功能。

**核心目标:**
- 实现 AIService 类和 DeepSeek API 客户端
- 实现在线状态检测和降级策略
- 实现文档摘要生成(200-300 字)
- 实现关键信息提取(人名、日期、金额、关键词)
- 实现摘要缓存和数据库持久化
- 支持批量处理和后台任务

## Acceptance Criteria

- [ ] AI 服务基础架构
  - **AIService 类实现**
    - 初始化配置(API Key, Base URL, 模型参数)
    - 在线状态检测(API 可用性测试)
    - API 调用封装(重试、超时、错误处理)
    - 请求限流(避免超过 API 配额)

  - **配置管理**
    - AI 功能开关(enable_ai)
    - DeepSeek API 配置(api_key, base_url)
    - 模型参数(model, temperature, max_tokens)
    - 超时和重试配置

- [ ] DeepSeek API 集成
  - **API 客户端实现**
    - HTTP 客户端配置(requests/httpx)
    - API 认证(Bearer Token)
    - 请求构建和响应解析
    - 错误处理(网络错误、API 错误、限流等)

  - **Prompt 工程**
    - 文档摘要 Prompt 模板
    - 关键信息提取 Prompt 模板
    - 中文优化的 Prompt
    - 结构化输出格式(JSON)

- [ ] 文档摘要生成
  - **摘要生成功能**
    - 提取文档核心内容(前 3000 字)
    - 调用 DeepSeek API 生成摘要
    - 摘要长度控制(200-300 字)
    - 摘要质量验证(非空、非错误信息)

  - **多文档类型支持**
    - Word/PDF/TXT 文档摘要
    - Excel 摘要(描述表格结构和内容)
    - PPT 摘要(概括演示内容)

- [ ] 关键信息提取
  - **实体提取**
    - 人名识别(中英文)
    - 日期提取(多种格式)
    - 金额提取(货币符号、大写金额)
    - 组织机构名称

  - **关键词提取**
    - 自动提取 5-10 个关键词
    - 关键词权重排序
    - 去重和规范化

- [ ] 缓存和持久化
  - **摘要缓存**
    - 内存缓存(LRU Cache)
    - 文件缓存(.cache/ai/)
    - 缓存键生成(文件路径 + 修改时间 + 内容哈希)
    - 缓存过期策略(30 天)

  - **数据库存储**
    - 扩展 documents 表(添加 summary, keywords, entities 字段)
    - 摘要数据持久化
    - 增量更新(只处理新文档或修改过的文档)

- [ ] 性能优化
  - 批量处理(多个文档合并请求)
  - 异步任务队列(后台生成摘要)
  - 超时保护(单次 API 调用最多 30 秒)
  - API 配额管理(每日限额检测)

- [ ] 降级和容错
  - 在线状态检测失败时禁用 AI 功能
  - API 调用失败时使用默认摘要(文档开头 200 字)
  - 网络错误自动重试(3 次)
  - 用户可手动禁用 AI 功能

- [ ] 单元测试
  - AIService 类功能测试
  - API 客户端 mock 测试
  - 缓存机制测试
  - 降级策略测试
  - 代码覆盖率 ≥ 85%

## Technical Details

### AI 服务类

```python
# src/services/ai_service.py
import requests
import hashlib
import json
import logging
import time
from typing import Optional, Dict, List, Any
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class SummaryResult:
    """摘要结果"""
    summary: str
    keywords: List[str]
    entities: Dict[str, List[str]]
    confidence: float
    processing_time: float
    from_cache: bool = False

class AIService:
    """AI 服务类"""

    def __init__(self, config: Dict[str, Any]):
        """
        初始化 AI 服务

        Args:
            config: 配置字典
                - api_key: DeepSeek API 密钥
                - base_url: API 基础 URL
                - model: 模型名称(默认 deepseek-chat)
                - temperature: 温度参数(默认 0.7)
                - max_tokens: 最大 token 数(默认 1000)
                - timeout: 超时时间(默认 30 秒)
                - enable_ai: 是否启用 AI 功能(默认 False)
        """
        self.api_key = config.get('api_key', '')
        self.base_url = config.get('base_url', 'https://api.deepseek.com/v1')
        self.model = config.get('model', 'deepseek-chat')
        self.temperature = config.get('temperature', 0.7)
        self.max_tokens = config.get('max_tokens', 1000)
        self.timeout = config.get('timeout', 30)
        self.enabled = config.get('enable_ai', False)

        # 缓存配置
        self.cache_dir = Path('.cache/ai')
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # 内存缓存(最多 100 个)
        from functools import lru_cache
        self._cache = {}
        self._cache_max_size = 100

        # 在线状态
        self._online = False

        if self.enabled:
            self._verify_connection()

    def _verify_connection(self) -> bool:
        """验证 API 连接"""
        if not self.api_key:
            logger.warning("DeepSeek API key not configured")
            self.enabled = False
            return False

        try:
            # 发送测试请求
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }

            test_payload = {
                'model': self.model,
                'messages': [
                    {'role': 'user', 'content': 'Hello'}
                ],
                'max_tokens': 10
            }

            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=test_payload,
                timeout=10
            )

            if response.status_code == 200:
                self._online = True
                logger.info("DeepSeek AI service is online")
                return True
            else:
                logger.error(f"DeepSeek API returned status {response.status_code}")
                self.enabled = False
                return False

        except Exception as e:
            logger.error(f"Failed to connect to DeepSeek API: {e}")
            self.enabled = False
            return False

    def is_online(self) -> bool:
        """检查 AI 服务是否在线"""
        return self.enabled and self._online

    def _call_api(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """
        调用 DeepSeek API

        Args:
            messages: 消息列表

        Returns:
            API 响应文本,失败返回 None
        """
        if not self.is_online():
            return None

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        payload = {
            'model': self.model,
            'messages': messages,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens
        }

        # 重试机制
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f'{self.base_url}/chat/completions',
                    headers=headers,
                    json=payload,
                    timeout=self.timeout
                )

                if response.status_code == 200:
                    data = response.json()
                    content = data['choices'][0]['message']['content']
                    return content.strip()

                elif response.status_code == 429:  # 限流
                    logger.warning("API rate limit reached, retrying...")
                    time.sleep(2 ** attempt)  # 指数退避
                    continue

                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")
                    return None

            except requests.Timeout:
                logger.warning(f"API timeout (attempt {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                return None

            except Exception as e:
                logger.error(f"API call failed: {e}")
                return None

        return None

    def _get_cache_key(self, content: str) -> str:
        """生成缓存键"""
        return hashlib.md5(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """从缓存加载"""
        # 检查内存缓存
        if cache_key in self._cache:
            return self._cache[cache_key]

        # 检查文件缓存
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # 检查缓存是否过期(30 天)
                    cache_time = data.get('timestamp', 0)
                    if time.time() - cache_time < 30 * 24 * 3600:
                        # 加载到内存缓存
                        self._cache[cache_key] = data
                        return data
            except Exception as e:
                logger.error(f"Failed to load cache: {e}")

        return None

    def _save_to_cache(self, cache_key: str, data: Dict[str, Any]):
        """保存到缓存"""
        # 保存到内存缓存
        if len(self._cache) >= self._cache_max_size:
            # 移除最旧的项(简单实现)
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]

        data['timestamp'] = time.time()
        self._cache[cache_key] = data

        # 保存到文件缓存
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")

    def generate_summary(self, content: str, doc_type: str = 'document') -> SummaryResult:
        """
        生成文档摘要

        Args:
            content: 文档内容
            doc_type: 文档类型(document, excel, ppt)

        Returns:
            SummaryResult 对象
        """
        start_time = time.time()

        # 检查缓存
        cache_key = self._get_cache_key(content)
        cached = self._load_from_cache(cache_key)

        if cached:
            logger.info("Using cached summary")
            return SummaryResult(
                summary=cached['summary'],
                keywords=cached['keywords'],
                entities=cached['entities'],
                confidence=cached['confidence'],
                processing_time=time.time() - start_time,
                from_cache=True
            )

        # 如果 AI 服务不可用,使用降级策略
        if not self.is_online():
            return self._generate_fallback_summary(content, start_time)

        # 截取内容(最多 3000 字)
        truncated_content = content[:3000]

        # 构建 Prompt
        prompt = self._build_summary_prompt(truncated_content, doc_type)

        messages = [
            {'role': 'system', 'content': '你是一个专业的文档分析助手,擅长提取文档关键信息并生成简洁的摘要。'},
            {'role': 'user', 'content': prompt}
        ]

        # 调用 API
        response = self._call_api(messages)

        if not response:
            # API 调用失败,使用降级策略
            return self._generate_fallback_summary(content, start_time)

        # 解析响应
        try:
            result = json.loads(response)
            summary = result.get('summary', '')
            keywords = result.get('keywords', [])
            entities = result.get('entities', {})

            # 验证结果
            if not summary or len(summary) < 50:
                raise ValueError("Invalid summary")

            # 保存到缓存
            cache_data = {
                'summary': summary,
                'keywords': keywords,
                'entities': entities,
                'confidence': 0.9
            }
            self._save_to_cache(cache_key, cache_data)

            return SummaryResult(
                summary=summary,
                keywords=keywords,
                entities=entities,
                confidence=0.9,
                processing_time=time.time() - start_time,
                from_cache=False
            )

        except Exception as e:
            logger.error(f"Failed to parse AI response: {e}")
            return self._generate_fallback_summary(content, start_time)

    def _build_summary_prompt(self, content: str, doc_type: str) -> str:
        """构建摘要生成 Prompt"""
        prompt = f"""请分析以下{doc_type}的内容,并以 JSON 格式返回分析结果:

文档内容:
{content}

请提供:
1. summary: 200-300 字的摘要,概括文档的核心内容和主要观点
2. keywords: 5-10 个关键词,按重要性排序
3. entities: 提取关键信息,包括:
   - persons: 人名列表
   - dates: 日期列表
   - amounts: 金额列表
   - organizations: 组织机构列表

返回格式:
{{
  "summary": "文档摘要...",
  "keywords": ["关键词1", "关键词2", ...],
  "entities": {{
    "persons": ["张三", "李四"],
    "dates": ["2024-01-01"],
    "amounts": ["10000元"],
    "organizations": ["XX公司"]
  }}
}}
"""
        return prompt

    def _generate_fallback_summary(self, content: str, start_time: float) -> SummaryResult:
        """生成降级摘要(不使用 AI)"""
        # 简单提取前 200 字作为摘要
        summary = content[:200].strip()
        if len(content) > 200:
            summary += "..."

        logger.info("Using fallback summary (AI service unavailable)")

        return SummaryResult(
            summary=summary,
            keywords=[],
            entities={},
            confidence=0.5,
            processing_time=time.time() - start_time,
            from_cache=False
        )

    def extract_keywords(self, content: str) -> List[str]:
        """
        提取关键词

        Args:
            content: 文档内容

        Returns:
            关键词列表
        """
        summary_result = self.generate_summary(content)
        return summary_result.keywords

    def extract_entities(self, content: str) -> Dict[str, List[str]]:
        """
        提取实体信息

        Args:
            content: 文档内容

        Returns:
            实体字典
        """
        summary_result = self.generate_summary(content)
        return summary_result.entities
```

### 数据库扩展

```python
# src/database/schema.py (扩展)
# 添加 AI 相关字段到 documents 表

CREATE_DOCUMENTS_TABLE = """
CREATE TABLE IF NOT EXISTS documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT NOT NULL UNIQUE,
    file_name TEXT NOT NULL,
    file_ext TEXT NOT NULL,
    file_size INTEGER,
    modified_time REAL,
    created_time REAL,
    content TEXT,
    content_hash TEXT,
    summary TEXT,              -- AI 生成的摘要
    keywords TEXT,             -- AI 提取的关键词(JSON 数组)
    entities TEXT,             -- AI 提取的实体(JSON 对象)
    ai_processed BOOLEAN DEFAULT 0,  -- 是否已进行 AI 处理
    ai_processed_time REAL,    -- AI 处理时间
    indexed_time REAL,
    INDEX idx_file_name ON documents(file_name),
    INDEX idx_file_ext ON documents(file_ext),
    INDEX idx_content_hash ON documents(content_hash),
    INDEX idx_ai_processed ON documents(ai_processed)
)
"""

# src/database/db_manager.py (扩展)
def save_ai_summary(
    self,
    file_path: str,
    summary: str,
    keywords: List[str],
    entities: Dict[str, List[str]]
):
    """保存 AI 摘要到数据库"""
    keywords_json = json.dumps(keywords, ensure_ascii=False)
    entities_json = json.dumps(entities, ensure_ascii=False)

    self.execute("""
        UPDATE documents
        SET summary = ?,
            keywords = ?,
            entities = ?,
            ai_processed = 1,
            ai_processed_time = ?
        WHERE file_path = ?
    """, (summary, keywords_json, entities_json, time.time(), file_path))
```

### 批量处理

```python
# src/services/ai_batch_processor.py
import logging
from typing import List
from concurrent.futures import ThreadPoolExecutor
from .ai_service import AIService

logger = logging.getLogger(__name__)

class AIBatchProcessor:
    """AI 批量处理器"""

    def __init__(self, ai_service: AIService, max_workers: int = 3):
        self.ai_service = ai_service
        self.max_workers = max_workers

    def process_documents(
        self,
        documents: List[Dict[str, str]],
        db_manager
    ):
        """
        批量处理文档

        Args:
            documents: 文档列表 [{'file_path': ..., 'content': ...}, ...]
            db_manager: 数据库管理器
        """
        if not self.ai_service.is_online():
            logger.warning("AI service is offline, skipping batch processing")
            return

        logger.info(f"Processing {len(documents)} documents with AI")

        def process_one(doc):
            try:
                result = self.ai_service.generate_summary(
                    doc['content'],
                    doc.get('doc_type', 'document')
                )

                if result.confidence > 0.7:
                    db_manager.save_ai_summary(
                        doc['file_path'],
                        result.summary,
                        result.keywords,
                        result.entities
                    )
                    logger.info(f"Processed: {doc['file_path']}")
                else:
                    logger.warning(f"Low confidence for: {doc['file_path']}")

            except Exception as e:
                logger.error(f"Failed to process {doc['file_path']}: {e}")

        # 使用线程池并行处理
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            executor.map(process_one, documents)

        logger.info("Batch processing completed")
```

## Dependencies

**外部依赖:**
- requests 2.31+
- httpx 0.25+ (可选,更好的异步支持)

**DeepSeek API:**
- API Key(需要注册账号)
- API 文档: https://platform.deepseek.com/docs

**内部依赖:**
- Task 003: 数据库层(扩展表结构)

## Effort Estimate

- **Size:** M (Medium)
- **Hours:** 10-14 小时
- **Parallel:** true (可与其他任务并行)

**时间分配:**
- AIService 类实现: 4 小时
- DeepSeek API 集成: 3 小时
- 缓存和持久化: 2 小时
- 批量处理实现: 2 小时
- 单元测试和调试: 3 小时

## Definition of Done

- [x] 代码实现完成
  - AIService 类实现
  - DeepSeek API 客户端
  - Prompt 工程
  - 缓存机制
  - 数据库集成
  - 批量处理器

- [x] 测试完成
  - API 调用测试(mock)
  - 摘要生成测试
  - 缓存功能测试
  - 降级策略测试
  - 代码覆盖率 ≥ 85%

- [x] 文档完成
  - API 配置文档
  - 使用指南
  - Prompt 模板说明
  - 最佳实践

- [x] 代码质量
  - 通过 black 格式化
  - 通过 pylint 检查(评分 ≥ 8.0)
  - 类型注解完整

- [x] 验收测试
  - AI 服务在线检测正常
  - 摘要生成质量良好
  - 关键信息提取准确
  - 缓存机制工作正常
  - 降级策略生效
  - 批量处理稳定

## Notes

**DeepSeek API 配置:**
```json
{
  "ai": {
    "enable_ai": true,
    "api_key": "sk-xxxxx",
    "base_url": "https://api.deepseek.com/v1",
    "model": "deepseek-chat",
    "temperature": 0.7,
    "max_tokens": 1000,
    "timeout": 30
  }
}
```

**API 费用:**
- DeepSeek API 按 token 计费
- 建议设置每日配额限制
- 监控 API 使用情况

**性能建议:**
- 优先处理常用文档
- 使用批量处理降低 API 调用频率
- 充分利用缓存减少重复请求
- 考虑离线模式(完全依赖降级策略)

**隐私和安全:**
- 文档内容通过 HTTPS 传输
- API Key 安全存储(不要硬编码)
- 敏感文档可禁用 AI 功能
- 遵守用户隐私政策
