---
name: 索引管理器实现
status: open
created: 2025-10-16T08:21:20Z
updated: 2025-10-16T08:21:20Z
github: Will be updated when synced to GitHub
depends_on: [002, 003]
parallel: false
conflicts_with: []
---

# Task: 索引管理器实现

## Description

实现高性能的索引管理器,负责文件扫描、内容提取、索引创建和增量更新。采用多进程并行处理提升大规模文件索引速度,通过文件哈希检测实现智能增量更新,避免重复索引。

**核心目标:**
- 实现 IndexManager 核心类
- 实现目录递归扫描和文件过滤
- 实现完整索引和增量索引
- 实现多进程并行处理(multiprocessing.Pool)
- 实现进度回调和错误处理
- 实现索引统计和日志记录

## Acceptance Criteria

- [ ] IndexManager 类实现
  - 单例模式(防止多实例冲突)
  - 集成 DatabaseManager
  - 集成 ParserFactory
  - 配置管理(工作进程数、批量大小等)

- [ ] 目录扫描功能
  - **递归扫描目录**
    - 使用 os.walk 遍历目录树
    - 支持多目录并行扫描
    - 排除系统目录和隐藏文件
    - 排除配置的扩展名和路径

  - **文件过滤**
    - 检查文件扩展名白名单
    - 排除配置的黑名单路径
    - 跳过符号链接和快捷方式
    - 文件大小限制检查(默认 < 100MB)

- [ ] 完整索引功能
  - 扫描目录获取所有支持的文件
  - 使用多进程并行解析文件
  - 批量插入数据库(每批 100 条)
  - 记录索引进度和统计信息
  - 错误文件记录但不中断流程
  - 支持进度回调(已处理/总数)

- [ ] 增量索引功能
  - **文件变更检测**
    - 计算文件 SHA256 哈希值
    - 比对数据库中的哈希值
    - 检测新增文件
    - 检测修改文件(哈希变化)
    - 检测删除文件(文件不存在)

  - **智能更新**
    - 仅索引新增和修改的文件
    - 删除已删除文件的索引
    - 更新文件修改时间
    - 更新索引统计信息

- [ ] 多进程并行处理
  - 使用 multiprocessing.Pool
  - 可配置工作进程数(默认为 CPU 核心数)
  - 进程间数据传递优化
  - 进程异常处理和恢复
  - 主进程协调和结果收集

- [ ] 进度回调机制
  - 定义 ProgressCallback 类型
  - 定时触发进度回调(每秒或每批次)
  - 回调参数: 已处理、总数、当前文件、错误数
  - 支持取消索引操作

- [ ] 异常处理和日志
  - 单个文件解析失败不影响整体
  - 记录详细错误日志(文件路径、错误信息)
  - 索引完成后生成统计报告
  - 数据库事务保护

## Technical Details

### IndexManager 核心类

```python
# src/core/index_manager.py
import os
import hashlib
import logging
import time
from pathlib import Path
from typing import List, Dict, Callable, Optional, Tuple
from multiprocessing import Pool, cpu_count
from dataclasses import dataclass

from ..data.database import DatabaseManager
from ..parsers.factory import ParserFactory
from ..utils.config import ConfigManager

logger = logging.getLogger(__name__)

@dataclass
class IndexStats:
    """索引统计信息"""
    total_files: int = 0
    indexed_files: int = 0
    failed_files: int = 0
    skipped_files: int = 0
    total_size: int = 0
    elapsed_time: float = 0.0
    errors: List[Tuple[str, str]] = None  # (file_path, error_message)

    def __post_init__(self):
        if self.errors is None:
            self.errors = []

# 进度回调类型
ProgressCallback = Callable[[int, int, str], None]

class IndexManager:
    """索引管理器"""

    _instance = None

    def __new__(cls):
        """单例模式"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, '_initialized'):
            self.config = ConfigManager()
            self.db = DatabaseManager()
            self.parser_factory = ParserFactory()

            # 配置参数
            self.parallel_workers = self.config.get('indexing.parallel_workers', cpu_count())
            self.batch_size = self.config.get('indexing.batch_size', 100)
            self.max_file_size = self.config.get('indexing.max_file_size_mb', 100) * 1024 * 1024
            self.excluded_extensions = self.config.get('indexing.excluded_extensions', [])
            self.excluded_paths = self.config.get('indexing.excluded_paths', [])

            self._initialized = True
            logger.info("IndexManager initialized")

    def create_index(
        self,
        directories: List[str],
        progress_callback: Optional[ProgressCallback] = None
    ) -> IndexStats:
        """
        创建完整索引

        Args:
            directories: 要索引的目录列表
            progress_callback: 进度回调函数(已处理, 总数, 当前文件)

        Returns:
            IndexStats: 索引统计信息
        """
        logger.info(f"Starting full indexing for {len(directories)} directories")
        start_time = time.time()
        stats = IndexStats()

        try:
            # 1. 扫描文件
            files = self._scan_directories(directories)
            stats.total_files = len(files)
            logger.info(f"Found {stats.total_files} files to index")

            if stats.total_files == 0:
                return stats

            # 2. 多进程并行解析
            results = self._parse_files_parallel(files, progress_callback)

            # 3. 批量插入数据库
            self._insert_results_batch(results, stats)

            # 4. 更新统计信息
            stats.elapsed_time = time.time() - start_time
            logger.info(f"Indexing completed: {stats.indexed_files}/{stats.total_files} files "
                       f"in {stats.elapsed_time:.2f}s")

            return stats

        except Exception as e:
            logger.error(f"Indexing failed: {e}")
            stats.elapsed_time = time.time() - start_time
            raise

    def update_index(
        self,
        directories: List[str],
        progress_callback: Optional[ProgressCallback] = None
    ) -> IndexStats:
        """
        增量更新索引

        Args:
            directories: 要更新的目录列表
            progress_callback: 进度回调函数

        Returns:
            IndexStats: 索引统计信息
        """
        logger.info(f"Starting incremental indexing for {len(directories)} directories")
        start_time = time.time()
        stats = IndexStats()

        try:
            # 1. 扫描当前文件
            current_files = self._scan_directories(directories)
            current_files_set = {f['path'] for f in current_files}
            stats.total_files = len(current_files)

            # 2. 获取已索引的文件
            indexed_files = self.db.get_all_file_paths()
            indexed_files_set = set(indexed_files)

            # 3. 检测变更
            new_files = []
            modified_files = []

            for file_info in current_files:
                file_path = file_info['path']
                file_hash = file_info['hash']

                if file_path not in indexed_files_set:
                    # 新文件
                    new_files.append(file_info)
                else:
                    # 检查哈希是否变化
                    old_hash = self.db.get_file_hash(file_path)
                    if old_hash != file_hash:
                        modified_files.append(file_info)

            # 4. 检测删除的文件
            deleted_files = indexed_files_set - current_files_set

            logger.info(f"Changes detected: {len(new_files)} new, "
                       f"{len(modified_files)} modified, {len(deleted_files)} deleted")

            # 5. 处理新增和修改的文件
            files_to_process = new_files + modified_files
            if files_to_process:
                results = self._parse_files_parallel(files_to_process, progress_callback)
                self._insert_results_batch(results, stats, update_mode=True)

            # 6. 删除已删除文件的索引
            for file_path in deleted_files:
                self.db.delete_document(file_path)

            stats.elapsed_time = time.time() - start_time
            logger.info(f"Incremental indexing completed in {stats.elapsed_time:.2f}s")

            return stats

        except Exception as e:
            logger.error(f"Incremental indexing failed: {e}")
            stats.elapsed_time = time.time() - start_time
            raise

    def _scan_directories(self, directories: List[str]) -> List[Dict]:
        """
        扫描目录,获取所有支持的文件

        Returns:
            List[Dict]: 文件信息列表 [{'path': ..., 'size': ..., 'hash': ...}, ...]
        """
        files = []

        for directory in directories:
            if not os.path.exists(directory):
                logger.warning(f"Directory not found: {directory}")
                continue

            for root, dirs, filenames in os.walk(directory):
                # 过滤排除的目录
                dirs[:] = [d for d in dirs if not self._is_excluded_path(os.path.join(root, d))]

                for filename in filenames:
                    file_path = os.path.join(root, filename)

                    # 过滤文件
                    if not self._should_index_file(file_path):
                        continue

                    try:
                        stat = os.stat(file_path)
                        file_info = {
                            'path': file_path,
                            'size': stat.st_size,
                            'modified': stat.st_mtime,
                            'hash': self._calculate_file_hash(file_path)
                        }
                        files.append(file_info)
                    except Exception as e:
                        logger.warning(f"Failed to stat file {file_path}: {e}")

        return files

    def _should_index_file(self, file_path: str) -> bool:
        """检查文件是否应该被索引"""
        # 检查扩展名
        ext = os.path.splitext(file_path)[1].lower()
        if ext in self.excluded_extensions:
            return False

        # 检查是否支持的格式
        if not self.parser_factory.supports(file_path):
            return False

        # 检查路径
        if self._is_excluded_path(file_path):
            return False

        # 检查文件大小
        try:
            size = os.path.getsize(file_path)
            if size > self.max_file_size:
                logger.debug(f"File too large: {file_path} ({size} bytes)")
                return False
        except:
            return False

        # 检查是否是符号链接
        if os.path.islink(file_path):
            return False

        return True

    def _is_excluded_path(self, path: str) -> bool:
        """检查路径是否在排除列表中"""
        path_lower = path.lower()
        for excluded in self.excluded_paths:
            if excluded.lower() in path_lower:
                return True
        return False

    def _calculate_file_hash(self, file_path: str) -> str:
        """计算文件 SHA256 哈希值"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                # 分块读取,避免大文件内存溢出
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""

    def _parse_files_parallel(
        self,
        files: List[Dict],
        progress_callback: Optional[ProgressCallback] = None
    ) -> List[Tuple[Dict, object]]:
        """
        多进程并行解析文件

        Returns:
            List[Tuple[file_info, parse_result]]: 解析结果列表
        """
        results = []
        total = len(files)

        with Pool(processes=self.parallel_workers) as pool:
            # 异步提交任务
            async_results = []
            for file_info in files:
                async_result = pool.apply_async(
                    _parse_file_worker,
                    (file_info['path'],)
                )
                async_results.append((file_info, async_result))

            # 收集结果
            for i, (file_info, async_result) in enumerate(async_results, 1):
                try:
                    parse_result = async_result.get(timeout=30)  # 单个文件最多30秒
                    results.append((file_info, parse_result))

                    # 触发进度回调
                    if progress_callback:
                        progress_callback(i, total, file_info['path'])

                except Exception as e:
                    logger.error(f"Failed to parse {file_info['path']}: {e}")
                    results.append((file_info, None))

        return results

    def _insert_results_batch(
        self,
        results: List[Tuple[Dict, object]],
        stats: IndexStats,
        update_mode: bool = False
    ):
        """批量插入解析结果到数据库"""
        batch = []

        for file_info, parse_result in results:
            if parse_result is None or not parse_result.success:
                stats.failed_files += 1
                if parse_result and parse_result.error:
                    stats.errors.append((file_info['path'], parse_result.error))
                continue

            # 准备文档数据
            doc_data = {
                'file_path': file_info['path'],
                'file_name': os.path.basename(file_info['path']),
                'file_size': file_info['size'],
                'file_hash': file_info['hash'],
                'modified_time': file_info['modified'],
                'content': parse_result.content,
                'metadata': parse_result.metadata
            }

            batch.append(doc_data)
            stats.total_size += file_info['size']

            # 批量插入
            if len(batch) >= self.batch_size:
                self._flush_batch(batch, update_mode)
                stats.indexed_files += len(batch)
                batch.clear()

        # 插入剩余数据
        if batch:
            self._flush_batch(batch, update_mode)
            stats.indexed_files += len(batch)

    def _flush_batch(self, batch: List[Dict], update_mode: bool = False):
        """批量插入数据库"""
        try:
            if update_mode:
                # 增量模式:先删除再插入
                for doc_data in batch:
                    self.db.delete_document(doc_data['file_path'])

            self.db.insert_documents_batch(batch)

        except Exception as e:
            logger.error(f"Failed to insert batch: {e}")
            raise

# 全局工作函数(必须在顶层,以便 pickle 序列化)
def _parse_file_worker(file_path: str):
    """工作进程解析文件(全局函数)"""
    try:
        from ..parsers.factory import ParserFactory
        factory = ParserFactory()
        parser = factory.get_parser(file_path)
        if parser:
            return parser.parse(file_path)
        return None
    except Exception as e:
        logger.error(f"Worker failed to parse {file_path}: {e}")
        return None
```

### 数据库扩展方法

```python
# src/data/database.py 新增方法

def get_all_file_paths(self) -> List[str]:
    """获取所有已索引的文件路径"""
    cursor = self.conn.execute("SELECT file_path FROM documents")
    return [row[0] for row in cursor.fetchall()]

def get_file_hash(self, file_path: str) -> Optional[str]:
    """获取文件哈希值"""
    cursor = self.conn.execute(
        "SELECT file_hash FROM documents WHERE file_path = ?",
        (file_path,)
    )
    row = cursor.fetchone()
    return row[0] if row else None

def delete_document(self, file_path: str):
    """删除文档索引"""
    self.conn.execute("DELETE FROM documents WHERE file_path = ?", (file_path,))
    self.conn.commit()

def insert_documents_batch(self, documents: List[Dict]):
    """批量插入文档"""
    for doc in documents:
        self.insert_document(
            file_path=doc['file_path'],
            file_name=doc['file_name'],
            content=doc['content'],
            file_size=doc['file_size'],
            file_hash=doc['file_hash'],
            modified_time=doc['modified_time'],
            metadata=doc.get('metadata', {})
        )
    self.conn.commit()
```

### 使用示例

```python
# 示例:创建完整索引
from src.core.index_manager import IndexManager

def progress_callback(processed, total, current_file):
    print(f"Progress: {processed}/{total} - {current_file}")

manager = IndexManager()
stats = manager.create_index(
    directories=['E:/Documents', 'E:/Projects'],
    progress_callback=progress_callback
)

print(f"Indexed: {stats.indexed_files}/{stats.total_files}")
print(f"Failed: {stats.failed_files}")
print(f"Total size: {stats.total_size / 1024 / 1024:.2f} MB")
print(f"Time: {stats.elapsed_time:.2f}s")

# 示例:增量更新索引
stats = manager.update_index(
    directories=['E:/Documents', 'E:/Projects'],
    progress_callback=progress_callback
)
```

## Dependencies

**外部依赖:**
- Python multiprocessing (标准库)
- hashlib (标准库)

**内部依赖:**
- Task 002: 数据库管理器
- Task 003: 文档解析引擎

## Effort Estimate

- **Size:** L (Large)
- **Hours:** 18-24 小时
- **Parallel:** false (依赖 Task 002 和 Task 003)

**时间分配:**
- IndexManager 核心框架: 4 小时
- 目录扫描和文件过滤: 3 小时
- 完整索引功能: 4 小时
- 增量索引功能: 5 小时
- 多进程并行处理: 4 小时
- 单元测试编写: 5 小时
- 文档和调试: 3 小时

## Definition of Done

- [ ] 代码实现完成
  - IndexManager 类实现(单例模式)
  - 目录扫描功能
  - 完整索引功能
  - 增量索引功能(哈希检测)
  - 多进程并行处理
  - 进度回调机制
  - 批量插入优化

- [ ] 测试完成
  - 单元测试覆盖率 >= 85%
  - 小规模文件测试(< 100 个文件)
  - 中等规模文件测试(1000+ 个文件)
  - 增量索引准确性测试
  - 多进程并发测试
  - 异常情况测试(文件损坏、权限不足等)

- [ ] 性能测试
  - 索引速度 >= 100 个文件/分钟(普通文档)
  - 多进程加速比 >= 2x(4核CPU)
  - 增量索引速度 >= 5x 完整索引
  - 内存占用 < 500MB(1000个文件)

- [ ] 文档完成
  - IndexManager API 文档
  - 配置参数说明
  - 使用示例代码
  - 性能优化建议

- [ ] 代码质量
  - 通过 black 格式化
  - 通过 pylint 检查(评分 >= 8.0)
  - 类型注解完整
  - 日志记录完善

- [ ] 验收测试
  - 能成功索引大量文件(1000+)
  - 增量索引正确检测变更
  - 进度回调正常工作
  - 索引统计信息准确
  - 多进程并行稳定运行
