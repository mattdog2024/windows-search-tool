---
name: 搜索引擎实现
status: open
created: 2025-10-16T08:21:20Z
updated: 2025-10-16T08:21:20Z
github: https://github.com/mattdog2024/windows-search-tool/issues/6
depends_on: [003]
parallel: true
conflicts_with: []
---

# Task: 搜索引擎实现

## Description

实现高性能的全文搜索引擎,基于 SQLite FTS5 提供快速准确的搜索功能。支持精确匹配和模糊搜索,利用 BM25 算法进行相关度排序,提供关键词高亮和上下文摘要,并通过 LRU 缓存提升重复查询性能。

**核心目标:**
- 实现 SearchEngine 核心类
- 实现 FTS5 查询构建(精确/模糊/布尔查询)
- 实现 BM25 相关度排序
- 实现关键词高亮和摘要提取
- 实现 LRU 结果缓存
- 实现搜索统计和性能监控

## Acceptance Criteria

- [ ] SearchEngine 类实现
  - 集成 DatabaseManager
  - 支持多种搜索模式
  - 结果分页支持
  - 搜索性能监控

- [ ] FTS5 查询构建
  - **精确匹配查询**
    - 完整词组匹配("文档管理系统")
    - 支持引号强制精确匹配
    - 大小写不敏感

  - **模糊搜索查询**
    - 支持通配符(*,?)
    - 支持前缀匹配(文档*)
    - 分词处理(中文分词)

  - **布尔查询**
    - AND 操作(文档 AND 管理)
    - OR 操作(文档 OR 系统)
    - NOT 操作(文档 NOT 临时)
    - 组合查询支持

  - **字段搜索**
    - 按文件名搜索
    - 按内容搜索
    - 按文件路径搜索

- [ ] 结果排序
  - **BM25 相关度排序**
    - 使用 FTS5 内置 BM25 算法
    - 词频(TF)权重
    - 逆文档频率(IDF)权重
    - 文档长度归一化

  - **辅助排序**
    - 文件修改时间排序
    - 文件大小排序
    - 文件路径排序
    - 组合排序(相关度 + 时间)

- [ ] 关键词高亮和摘要
  - **关键词高亮**
    - 标记搜索关键词(使用 <mark> 标签)
    - 支持多个关键词高亮
    - 保留上下文

  - **摘要提取**
    - 提取包含关键词的文本片段
    - 摘要长度可配置(默认 200 字符)
    - 多个片段用 "..." 连接
    - 优先显示关键词密度高的片段

- [ ] 结果缓存
  - **LRU 缓存实现**
    - 使用 functools.lru_cache
    - 缓存大小可配置(默认 100)
    - 缓存键:查询字符串 + 参数
    - 缓存失效策略

  - **缓存管理**
    - 手动清除缓存
    - 索引更新时自动清除
    - 缓存命中率统计

- [ ] 分页支持
  - 支持 offset 和 limit
  - 总结果数统计
  - 分页元数据(总页数、当前页等)

- [ ] 搜索统计
  - 查询耗时记录
  - 结果数量统计
  - 缓存命中率
  - 热门搜索关键词

## Technical Details

### SearchEngine 核心类

```python
# src/core/search_engine.py
import re
import logging
import time
from typing import List, Dict, Optional, Tuple
from functools import lru_cache
from dataclasses import dataclass

from ..data.database import DatabaseManager
from ..utils.config import ConfigManager

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """搜索结果"""
    file_path: str
    file_name: str
    file_size: int
    modified_time: float
    snippet: str  # 高亮摘要
    relevance_score: float
    metadata: Dict

@dataclass
class SearchResponse:
    """搜索响应"""
    results: List[SearchResult]
    total: int
    query: str
    elapsed_time: float
    page: int
    page_size: int
    total_pages: int
    cache_hit: bool = False

class SearchEngine:
    """搜索引擎"""

    def __init__(self):
        self.config = ConfigManager()
        self.db = DatabaseManager()

        # 配置参数
        self.default_page_size = self.config.get('search.results_per_page', 20)
        self.snippet_length = self.config.get('search.snippet_length', 200)
        self.cache_size = self.config.get('search.cache_size', 100)

        # 统计信息
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'total_results': 0,
            'avg_query_time': 0.0
        }

        logger.info("SearchEngine initialized")

    def search(
        self,
        query: str,
        mode: str = 'fuzzy',  # 'exact', 'fuzzy', 'boolean'
        field: Optional[str] = None,  # 'file_name', 'content', 'file_path'
        page: int = 1,
        page_size: Optional[int] = None,
        sort_by: str = 'relevance',  # 'relevance', 'modified_time', 'file_size'
        use_cache: bool = True
    ) -> SearchResponse:
        """
        执行搜索

        Args:
            query: 搜索关键词
            mode: 搜索模式(exact/fuzzy/boolean)
            field: 指定搜索字段
            page: 页码(从1开始)
            page_size: 每页结果数
            sort_by: 排序方式
            use_cache: 是否使用缓存

        Returns:
            SearchResponse: 搜索响应
        """
        start_time = time.time()
        page_size = page_size or self.default_page_size

        # 更新统计
        self.stats['total_queries'] += 1

        try:
            # 缓存检查
            cache_key = self._make_cache_key(query, mode, field, page, page_size, sort_by)
            if use_cache:
                cached_result = self._get_from_cache(cache_key)
                if cached_result:
                    self.stats['cache_hits'] += 1
                    cached_result.cache_hit = True
                    cached_result.elapsed_time = time.time() - start_time
                    return cached_result

            # 构建 FTS5 查询
            fts_query = self._build_fts_query(query, mode, field)

            # 执行数据库查询
            results, total = self._execute_search(
                fts_query, page, page_size, sort_by
            )

            # 处理结果(高亮、摘要)
            processed_results = self._process_results(results, query)

            # 构建响应
            response = SearchResponse(
                results=processed_results,
                total=total,
                query=query,
                elapsed_time=time.time() - start_time,
                page=page,
                page_size=page_size,
                total_pages=(total + page_size - 1) // page_size,
                cache_hit=False
            )

            # 更新统计
            self.stats['total_results'] += total
            self._update_avg_query_time(response.elapsed_time)

            # 缓存结果
            if use_cache:
                self._put_to_cache(cache_key, response)

            logger.info(f"Search completed: '{query}' - {total} results in {response.elapsed_time:.3f}s")
            return response

        except Exception as e:
            logger.error(f"Search failed: {e}")
            raise

    def _build_fts_query(self, query: str, mode: str, field: Optional[str]) -> str:
        """
        构建 FTS5 查询语句

        FTS5 查询语法:
        - "phrase" : 精确短语匹配
        - word* : 前缀匹配
        - word1 AND word2 : 布尔 AND
        - word1 OR word2 : 布尔 OR
        - word1 NOT word2 : 布尔 NOT
        - {field}: query : 字段搜索
        """
        # 转义特殊字符
        escaped_query = self._escape_fts_query(query)

        if mode == 'exact':
            # 精确匹配:使用引号
            fts_query = f'"{escaped_query}"'

        elif mode == 'fuzzy':
            # 模糊匹配:分词 + 前缀匹配
            words = escaped_query.split()
            fts_query = ' OR '.join(f'{word}*' for word in words)

        elif mode == 'boolean':
            # 布尔查询:直接使用用户输入(已转义)
            fts_query = escaped_query

        else:
            raise ValueError(f"Invalid search mode: {mode}")

        # 字段限定
        if field:
            fts_query = f'{{{field}}}: {fts_query}'

        return fts_query

    def _escape_fts_query(self, query: str) -> str:
        """转义 FTS5 特殊字符"""
        # FTS5 特殊字符: " * ( ) 等
        # 保留用户输入的 AND, OR, NOT
        special_chars = ['"']
        for char in special_chars:
            query = query.replace(char, f'\\{char}')
        return query

    def _execute_search(
        self,
        fts_query: str,
        page: int,
        page_size: int,
        sort_by: str
    ) -> Tuple[List[Dict], int]:
        """
        执行数据库搜索

        Returns:
            (results, total): 结果列表和总数
        """
        # 构建 SQL 查询
        sql = f"""
            SELECT
                d.file_path,
                d.file_name,
                d.file_size,
                d.modified_time,
                d.metadata,
                f.content,
                bm25(f) as relevance_score
            FROM documents_fts f
            JOIN documents d ON f.rowid = d.rowid
            WHERE f MATCH ?
        """

        # 排序
        if sort_by == 'relevance':
            sql += " ORDER BY relevance_score"
        elif sort_by == 'modified_time':
            sql += " ORDER BY d.modified_time DESC"
        elif sort_by == 'file_size':
            sql += " ORDER BY d.file_size DESC"
        else:
            sql += " ORDER BY relevance_score"

        # 分页
        offset = (page - 1) * page_size
        sql += f" LIMIT {page_size} OFFSET {offset}"

        # 执行查询
        cursor = self.db.conn.execute(sql, (fts_query,))
        results = []

        for row in cursor.fetchall():
            results.append({
                'file_path': row[0],
                'file_name': row[1],
                'file_size': row[2],
                'modified_time': row[3],
                'metadata': row[4],
                'content': row[5],
                'relevance_score': row[6]
            })

        # 统计总数
        count_sql = """
            SELECT COUNT(*)
            FROM documents_fts
            WHERE documents_fts MATCH ?
        """
        cursor = self.db.conn.execute(count_sql, (fts_query,))
        total = cursor.fetchone()[0]

        return results, total

    def _process_results(self, results: List[Dict], query: str) -> List[SearchResult]:
        """处理结果:高亮、摘要提取"""
        processed = []

        for result in results:
            # 提取摘要和高亮
            snippet = self._extract_snippet(
                result['content'],
                query,
                self.snippet_length
            )

            search_result = SearchResult(
                file_path=result['file_path'],
                file_name=result['file_name'],
                file_size=result['file_size'],
                modified_time=result['modified_time'],
                snippet=snippet,
                relevance_score=abs(result['relevance_score']),  # BM25 分数为负
                metadata=result['metadata']
            )
            processed.append(search_result)

        return processed

    def _extract_snippet(self, content: str, query: str, max_length: int) -> str:
        """
        提取包含关键词的摘要片段,并高亮关键词

        策略:
        1. 查找所有关键词位置
        2. 选择关键词密度最高的区域
        3. 提取上下文
        4. 高亮关键词
        """
        if not content:
            return ""

        # 提取查询词(忽略 AND, OR, NOT 等布尔操作符)
        keywords = self._extract_keywords(query)
        if not keywords:
            return content[:max_length] + "..."

        # 查找关键词位置
        positions = []
        for keyword in keywords:
            # 不区分大小写
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            for match in pattern.finditer(content):
                positions.append((match.start(), match.end(), keyword))

        if not positions:
            return content[:max_length] + "..."

        # 选择关键词密度最高的区域
        best_start, best_end = self._find_best_snippet_range(
            positions, len(content), max_length
        )

        # 提取片段
        snippet = content[best_start:best_end]

        # 高亮关键词
        snippet = self._highlight_keywords(snippet, keywords)

        # 添加省略号
        if best_start > 0:
            snippet = "..." + snippet
        if best_end < len(content):
            snippet = snippet + "..."

        return snippet

    def _extract_keywords(self, query: str) -> List[str]:
        """从查询中提取关键词"""
        # 移除布尔操作符
        query = re.sub(r'\b(AND|OR|NOT)\b', '', query, flags=re.IGNORECASE)
        # 移除特殊字符
        query = re.sub(r'[*"()]', '', query)
        # 分词
        keywords = [word.strip() for word in query.split() if word.strip()]
        return keywords

    def _find_best_snippet_range(
        self,
        positions: List[Tuple[int, int, str]],
        content_length: int,
        max_length: int
    ) -> Tuple[int, int]:
        """查找关键词密度最高的区域"""
        if not positions:
            return 0, min(max_length, content_length)

        # 简单策略:以第一个关键词为中心
        first_pos = positions[0][0]
        start = max(0, first_pos - max_length // 2)
        end = min(content_length, start + max_length)

        # 调整到词边界
        return start, end

    def _highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """高亮关键词"""
        for keyword in keywords:
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            text = pattern.sub(lambda m: f'<mark>{m.group(0)}</mark>', text)
        return text

    def _make_cache_key(self, query: str, mode: str, field: Optional[str],
                        page: int, page_size: int, sort_by: str) -> str:
        """生成缓存键"""
        return f"{query}|{mode}|{field}|{page}|{page_size}|{sort_by}"

    @lru_cache(maxsize=100)
    def _get_from_cache(self, cache_key: str) -> Optional[SearchResponse]:
        """从缓存获取(装饰器实现)"""
        # 实际缓存由 lru_cache 管理
        return None

    def _put_to_cache(self, cache_key: str, response: SearchResponse):
        """放入缓存"""
        # 实际缓存由 lru_cache 管理
        pass

    def clear_cache(self):
        """清除缓存"""
        self._get_from_cache.cache_clear()
        logger.info("Search cache cleared")

    def _update_avg_query_time(self, query_time: float):
        """更新平均查询时间"""
        total = self.stats['total_queries']
        avg = self.stats['avg_query_time']
        self.stats['avg_query_time'] = (avg * (total - 1) + query_time) / total

    def get_stats(self) -> Dict:
        """获取搜索统计信息"""
        cache_hit_rate = (self.stats['cache_hits'] / self.stats['total_queries'] * 100
                         if self.stats['total_queries'] > 0 else 0)

        return {
            'total_queries': self.stats['total_queries'],
            'cache_hits': self.stats['cache_hits'],
            'cache_hit_rate': f"{cache_hit_rate:.2f}%",
            'total_results': self.stats['total_results'],
            'avg_query_time': f"{self.stats['avg_query_time']:.3f}s"
        }
```

### 使用示例

```python
# 示例 1: 基础搜索
from src.core.search_engine import SearchEngine

engine = SearchEngine()

# 模糊搜索
response = engine.search(
    query="项目管理",
    mode='fuzzy',
    page=1,
    page_size=10
)

print(f"找到 {response.total} 个结果 (耗时: {response.elapsed_time:.3f}s)")
for result in response.results:
    print(f"文件: {result.file_name}")
    print(f"相关度: {result.relevance_score:.2f}")
    print(f"摘要: {result.snippet}")
    print("-" * 50)

# 示例 2: 精确搜索
response = engine.search(
    query="Windows Search Tool",
    mode='exact',
    page=1
)

# 示例 3: 布尔搜索
response = engine.search(
    query="文档 AND 管理 NOT 临时",
    mode='boolean',
    page=1
)

# 示例 4: 字段搜索
response = engine.search(
    query="报告",
    mode='fuzzy',
    field='file_name',  # 仅搜索文件名
    page=1
)

# 示例 5: 排序
response = engine.search(
    query="项目",
    mode='fuzzy',
    sort_by='modified_time',  # 按修改时间排序
    page=1
)

# 示例 6: 查看统计信息
stats = engine.get_stats()
print(f"查询统计: {stats}")

# 示例 7: 清除缓存
engine.clear_cache()
```

## Dependencies

**外部依赖:**
- SQLite FTS5 (内置)
- functools.lru_cache (标准库)

**内部依赖:**
- Task 003: 数据库管理器(FTS5 索引)

## Effort Estimate

- **Size:** M (Medium)
- **Hours:** 12-16 小时
- **Parallel:** true (可与 Task 004 并行开发)

**时间分配:**
- SearchEngine 核心框架: 3 小时
- FTS5 查询构建: 3 小时
- 结果排序实现: 2 小时
- 关键词高亮和摘要: 4 小时
- 结果缓存: 2 小时
- 单元测试编写: 4 小时
- 文档和调试: 2 小时

## Definition of Done

- [ ] 代码实现完成
  - SearchEngine 类实现
  - FTS5 查询构建(精确/模糊/布尔)
  - BM25 相关度排序
  - 关键词高亮和摘要提取
  - LRU 缓存实现
  - 分页支持
  - 搜索统计

- [ ] 测试完成
  - 单元测试覆盖率 >= 90%
  - 各种查询模式测试
  - 中文关键词测试
  - 特殊字符处理测试
  - 分页功能测试
  - 缓存功能测试

- [ ] 性能测试
  - 查询速度 < 100ms(小型数据库,1000文档)
  - 查询速度 < 500ms(中型数据库,10000文档)
  - 缓存命中率 >= 30%(典型使用场景)
  - BM25 排序准确性验证

- [ ] 文档完成
  - SearchEngine API 文档
  - 查询语法说明
  - 使用示例代码
  - 性能优化建议

- [ ] 代码质量
  - 通过 black 格式化
  - 通过 pylint 检查(评分 >= 8.0)
  - 类型注解完整
  - 日志记录完善

- [ ] 验收测试
  - 能正确执行各种查询模式
  - 搜索结果相关度排序合理
  - 关键词高亮显示正确
  - 摘要提取准确
  - 分页功能正常
  - 缓存有效提升性能
